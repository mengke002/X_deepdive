#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æç»“æœå¯¼å‡ºè„šæœ¬
ä» ANALYSIS_DATABASE_URL æ•°æ®åº“ä¸­å¯¼å‡ºåˆ†æç»“æœåˆ°æœ¬åœ°æ–‡ä»¶

åŠŸèƒ½ç‰¹æ€§:
    - äº¤äº’å¼èœå•é€‰æ‹©å¯¼å‡ºå†…å®¹
    - æ”¯æŒå¤šç§å¯¼å‡ºæ ¼å¼ (CSV, JSON, Markdown)
    - å½©è‰²ç»ˆç«¯è¾“å‡ºï¼Œå‹å¥½çš„ç”¨æˆ·ä½“éªŒ
    - è‡ªåŠ¨ç”Ÿæˆå¯è¯»çš„åˆ†ææŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
    python -m src.export_results                    # äº¤äº’å¼èœå•
    python -m src.export_results --session SESSION_ID --output ./exports
    python -m src.export_results --latest --output ./exports
    python -m src.export_results --list             # åˆ—å‡ºæ‰€æœ‰ä¼šè¯
    python -m src.export_results --all              # å¯¼å‡ºæ‰€æœ‰æ•°æ®
    python -m src.export_results --format markdown  # æŒ‡å®šå¯¼å‡ºæ ¼å¼
"""

import os
import sys
import json
import argparse
import logging
from datetime import datetime
from typing import Optional, List, Dict, Any, Tuple
from pathlib import Path

import pandas as pd

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# =====================================================
# ç»ˆç«¯é¢œè‰²å’Œæ ·å¼
# =====================================================
class Colors:
    """ç»ˆç«¯é¢œè‰²å®šä¹‰"""
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    
    @classmethod
    def disable(cls):
        """åœ¨ä¸æ”¯æŒé¢œè‰²çš„ç»ˆç«¯ä¸­ç¦ç”¨é¢œè‰²"""
        cls.HEADER = ''
        cls.BLUE = ''
        cls.CYAN = ''
        cls.GREEN = ''
        cls.YELLOW = ''
        cls.RED = ''
        cls.ENDC = ''
        cls.BOLD = ''
        cls.UNDERLINE = ''


def print_banner():
    """æ‰“å°æ¬¢è¿æ¨ªå¹…"""
    banner = f"""
{Colors.CYAN}{Colors.BOLD}
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                  â•‘
â•‘        ğŸ”  X_deepdive åˆ†æç»“æœå¯¼å‡ºå·¥å…·  ğŸ“Š                        â•‘
â•‘                                                                  â•‘
â•‘   ä»åˆ†ææ•°æ®åº“ä¸­æå–å¹¶æ ¼å¼åŒ–è¾“å‡ºæ‰€æœ‰æ·±åº¦åˆ†æç»“æœ                    â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{Colors.ENDC}
"""
    print(banner)


def print_section(title: str, icon: str = "ğŸ“"):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print(f"\n{Colors.YELLOW}{Colors.BOLD}{icon} {title}{Colors.ENDC}")
    print(f"{Colors.YELLOW}{'â”€' * 60}{Colors.ENDC}")


def print_success(message: str):
    """æ‰“å°æˆåŠŸæ¶ˆæ¯"""
    print(f"{Colors.GREEN}âœ… {message}{Colors.ENDC}")


def print_error(message: str):
    """æ‰“å°é”™è¯¯æ¶ˆæ¯"""
    print(f"{Colors.RED}âŒ {message}{Colors.ENDC}")


def print_info(message: str):
    """æ‰“å°ä¿¡æ¯æ¶ˆæ¯"""
    print(f"{Colors.BLUE}â„¹ï¸  {message}{Colors.ENDC}")


def print_warning(message: str):
    """æ‰“å°è­¦å‘Šæ¶ˆæ¯"""
    print(f"{Colors.YELLOW}âš ï¸  {message}{Colors.ENDC}")


# =====================================================
# é…ç½®åŠ è½½
# =====================================================
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        from .config import config
        return config
    except ImportError:
        # ä½œä¸ºç‹¬ç«‹è„šæœ¬è¿è¡Œæ—¶
        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        from src.config import config
        return config


# =====================================================
# æ•°æ®åº“è¿æ¥
# =====================================================
def get_analysis_connection():
    """è·å–åˆ†ææ•°æ®åº“è¿æ¥"""
    # ä¼˜å…ˆä»é…ç½®æ–‡ä»¶è¯»å–
    try:
        cfg = load_config()
        db_config = cfg.get_analysis_database_config()
        if db_config:
            import pymysql
            # æ·»åŠ è¶…æ—¶è®¾ç½®ï¼Œé¿å…åœ¨ CI ç¯å¢ƒä¸­æ— é™ç­‰å¾…
            db_config['connect_timeout'] = 10
            db_config['read_timeout'] = 60
            db_config['write_timeout'] = 60
            connection = pymysql.connect(**db_config)
            print_success(f"è¿æ¥åˆ°åˆ†ææ•°æ®åº“: {db_config['host']}:{db_config['port']}/{db_config['database']}")
            return connection
    except Exception as e:
        logger.debug(f"ä»é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
    
    # å›é€€åˆ°ç¯å¢ƒå˜é‡
    db_uri = os.getenv('ANALYSIS_DATABASE_URL')
    
    if not db_uri:
        print_error("æœªé…ç½®åˆ†ææ•°æ®åº“è¿æ¥")
        print_info("è¯·åœ¨ config.ini ä¸­é…ç½® ANALYSIS_DATABASE_URL æˆ–è®¾ç½®ç¯å¢ƒå˜é‡")
        print_info("ç¤ºä¾‹: export ANALYSIS_DATABASE_URL='mysql://user:pass@host:port/dbname'")
        return None
    
    import re
    pattern = r'mysql://([^:]+):([^@]+)@([^:]+):(\d+)/([^?]+)(\?.*)?'
    match = re.match(pattern, db_uri)
    
    if not match:
        print_error(f"æ— æ³•è§£ææ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²")
        return None
    
    user, password, host, port, database, params = match.groups()
    
    try:
        import pymysql
        config = {
            'host': host,
            'port': int(port),
            'user': user,
            'password': password,
            'database': database,
            'charset': 'utf8mb4',
            'autocommit': True,
        }
        
        # è‡ªåŠ¨é…ç½® SSL (TiDB Cloud å¼ºåˆ¶è¦æ±‚)
        if (params and 'ssl-mode=REQUIRED' in params) or 'tidbcloud.com' in host:
            config['ssl'] = {}
            # å°è¯•æŸ¥æ‰¾ç³»ç»ŸCAè¯ä¹¦
            possible_paths = [
                '/etc/ssl/certs/ca-certificates.crt',  # Debian/Ubuntu
                '/etc/pki/tls/certs/ca-bundle.crt',    # Fedora/RHEL
                '/etc/ssl/cert.pem',                   # macOS
                '/usr/local/etc/openssl/cert.pem',     # macOS Homebrew
            ]
            for path in possible_paths:
                if os.path.exists(path):
                    config['ssl']['ca'] = path
                    break
        
        # æ·»åŠ è¶…æ—¶è®¾ç½®ï¼Œé¿å…åœ¨ CI ç¯å¢ƒä¸­æ— é™ç­‰å¾…
        config['connect_timeout'] = 10
        config['read_timeout'] = 60
        config['write_timeout'] = 60
        
        connection = pymysql.connect(**config)
        print_success(f"è¿æ¥åˆ°åˆ†ææ•°æ®åº“: {host}:{port}/{database}")
        return connection
    except ImportError:
        print_error("æœªå®‰è£… pymysqlï¼Œè¯·è¿è¡Œ: pip install pymysql")
        return None
    except Exception as e:
        print_error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return None


# =====================================================
# ä¼šè¯ç®¡ç†
# =====================================================
def list_sessions(connection) -> pd.DataFrame:
    """åˆ—å‡ºæ‰€æœ‰åˆ†æä¼šè¯"""
    query = """
    SELECT 
        session_id,
        started_at,
        completed_at,
        status,
        TIMESTAMPDIFF(SECOND, started_at, COALESCE(completed_at, NOW())) as duration_seconds
    FROM analysis_sessions
    ORDER BY started_at DESC
    LIMIT 20
    """
    
    df = pd.read_sql(query, connection)
    return df


def get_latest_session(connection) -> Optional[str]:
    """è·å–æœ€æ–°å®Œæˆçš„ä¼šè¯ID"""
    query = """
    SELECT session_id 
    FROM analysis_sessions 
    WHERE status = 'completed'
    ORDER BY completed_at DESC 
    LIMIT 1
    """
    
    df = pd.read_sql(query, connection)
    if df.empty:
        return None
    return df.iloc[0]['session_id']


def get_session_info(connection, session_id: str) -> Optional[Dict]:
    """è·å–ä¼šè¯è¯¦ç»†ä¿¡æ¯"""
    query = """
    SELECT * FROM analysis_sessions WHERE session_id = %s
    """
    df = pd.read_sql(query, connection, params=[session_id])
    if df.empty:
        return None
    return df.iloc[0].to_dict()


def get_session_stats(connection, session_id: str) -> Dict[str, int]:
    """è·å–ä¼šè¯æ•°æ®ç»Ÿè®¡"""
    stats = {}
    tables = [
        ('user_metrics', 'ç”¨æˆ·æŒ‡æ ‡'),
        ('community_stats', 'ç¤¾ç¾¤ç»Ÿè®¡'),
        ('strong_ties', 'å¼ºäº’æƒ å…³ç³»'),
        ('content_outliers', 'é«˜ä»·å€¼å†…å®¹'),
        ('activity_stats', 'æ´»è·ƒåº¦ç»Ÿè®¡'),
        ('potential_new_users', 'æ½œåœ¨æ–°ç”¨æˆ·'),
        ('llm_outputs', 'LLMè¾“å‡ºè®°å½•'),
        ('user_strategy_dossiers', 'ç”¨æˆ·ç­–ç•¥ç”»åƒ'),
        ('content_blueprints', 'çˆ†æ¬¾å†…å®¹è“å›¾'),
        ('content_idea_bank', 'å†…å®¹åˆ›æ„åº“'),
        ('post_features', 'æ¨æ–‡ç‰¹å¾'),
        ('content_efficiency', 'å†…å®¹æ•ˆèƒ½')
    ]
    
    cursor = connection.cursor()
    for table, name in tables:
        try:
            cursor.execute(f"SELECT COUNT(*) FROM {table} WHERE session_id = %s", [session_id])
            count = cursor.fetchone()[0]
            stats[name] = count
        except Exception:
            stats[name] = 0
    cursor.close()
    
    return stats


# =====================================================
# æ•°æ®å¯¼å‡ºå‡½æ•°
# =====================================================
def export_user_metrics(connection, session_id: str, output_dir: str, fmt: str = 'csv') -> Tuple[pd.DataFrame, List[str]]:
    """
    å¯¼å‡ºç”¨æˆ·æŒ‡æ ‡æ•°æ®
    
    Returns:
        Tuple[DataFrame, List[str]]: æ•°æ®æ¡†å’Œç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
    """
    query = """
    SELECT 
        um.username,
        um.pagerank,
        um.betweenness,
        um.in_degree,
        um.community_id,
        um.talkativity_ratio,
        um.professionalism_index,
        um.avg_reply_latency_seconds,
        um.rising_star_velocity,
        um.avg_utility_score,
        um.category,
        um.analysis_timestamp
    FROM user_metrics um
    WHERE um.session_id = %s
    ORDER BY um.pagerank DESC
    """
    
    df = pd.read_sql(query, connection, params=[session_id])
    files_created = []
    
    if not df.empty:
        # å®Œæ•´æ•°æ®
        filepath = os.path.join(output_dir, f'all_users_with_metrics.{fmt}')
        _save_dataframe(df, filepath, fmt)
        files_created.append(filepath)
        print_success(f"å¯¼å‡ºç”¨æˆ·æŒ‡æ ‡: {os.path.basename(filepath)} ({len(df)} æ¡)")
        
        # æƒå¨æ¢çº½æ¸…å• (Top PageRank)
        authorities = df.nlargest(50, 'pagerank')
        filepath = os.path.join(output_dir, f'watchlist_authorities.{fmt}')
        _save_dataframe(authorities, filepath, fmt)
        files_created.append(filepath)
        print_info(f"  â””â”€ æƒå¨æ¢çº½ Top 50: {os.path.basename(filepath)}")
        
        # ç ´åœˆè€…æ¸…å• (Top Betweenness)
        connectors = df.nlargest(50, 'betweenness')
        filepath = os.path.join(output_dir, f'watchlist_connectors.{fmt}')
        _save_dataframe(connectors, filepath, fmt)
        files_created.append(filepath)
        print_info(f"  â””â”€ ç ´åœˆè€… Top 50: {os.path.basename(filepath)}")
        
        # å´›èµ·æ–°æ˜Ÿæ¸…å• (Top Rising Star Velocity)
        rising_stars = df[df['rising_star_velocity'] > 0].nlargest(50, 'rising_star_velocity')
        if not rising_stars.empty:
            filepath = os.path.join(output_dir, f'watchlist_rising_stars.{fmt}')
            _save_dataframe(rising_stars, filepath, fmt)
            files_created.append(filepath)
            print_info(f"  â””â”€ å´›èµ·æ–°æ˜Ÿ Top 50: {os.path.basename(filepath)}")
    
    return df, files_created


def export_community_stats(connection, session_id: str, output_dir: str, fmt: str = 'csv') -> Tuple[pd.DataFrame, List[str]]:
    """å¯¼å‡ºç¤¾ç¾¤ç»Ÿè®¡æ•°æ®"""
    query = """
    SELECT 
        community_id,
        member_count,
        avg_pagerank,
        avg_betweenness,
        total_followers,
        top_members_json,
        topic_keywords_json,
        analysis_timestamp
    FROM community_stats
    WHERE session_id = %s
    ORDER BY member_count DESC
    """
    
    df = pd.read_sql(query, connection, params=[session_id])
    files_created = []
    
    if not df.empty:
        filepath = os.path.join(output_dir, f'community_stats.{fmt}')
        _save_dataframe(df, filepath, fmt)
        files_created.append(filepath)
        print_success(f"å¯¼å‡ºç¤¾ç¾¤ç»Ÿè®¡: {os.path.basename(filepath)} ({len(df)} ä¸ªç¤¾ç¾¤)")
    
    return df, files_created


def export_strong_ties(connection, session_id: str, output_dir: str, fmt: str = 'csv') -> Tuple[pd.DataFrame, List[str]]:
    """å¯¼å‡ºå¼ºäº’æƒ å…³ç³»æ•°æ®"""
    query = """
    SELECT 
        user_a,
        user_b,
        interaction_weight,
        interaction_samples_json,
        relationship_type,
        analysis_timestamp
    FROM strong_ties
    WHERE session_id = %s
    ORDER BY interaction_weight DESC
    """
    
    df = pd.read_sql(query, connection, params=[session_id])
    files_created = []
    
    if not df.empty:
        filepath = os.path.join(output_dir, f'list_interactions_strong_ties.{fmt}')
        _save_dataframe(df, filepath, fmt)
        files_created.append(filepath)
        print_success(f"å¯¼å‡ºå¼ºäº’æƒ å…³ç³»: {os.path.basename(filepath)} ({len(df)} å¯¹)")
    
    return df, files_created


def export_content_outliers(connection, session_id: str, output_dir: str, fmt: str = 'csv') -> Tuple[pd.DataFrame, List[str]]:
    """å¯¼å‡ºé«˜ä»·å€¼å†…å®¹æ•°æ®"""
    query = """
    SELECT 
        tweet_id,
        author,
        text,
        created_at,
        outlier_type,
        score,
        analysis_timestamp
    FROM content_outliers
    WHERE session_id = %s
    ORDER BY score DESC
    """
    
    df = pd.read_sql(query, connection, params=[session_id])
    files_created = []
    
    if not df.empty:
        # æ‰€æœ‰å¼‚å¸¸å†…å®¹
        filepath = os.path.join(output_dir, f'list_posts_outliers.{fmt}')
        _save_dataframe(df, filepath, fmt)
        files_created.append(filepath)
        print_success(f"å¯¼å‡ºé«˜ä»·å€¼å†…å®¹: {os.path.basename(filepath)} ({len(df)} æ¡)")
        
        # æŒ‰ç±»å‹åˆ†ç±»å¯¼å‡º
        for outlier_type in df['outlier_type'].dropna().unique():
            type_df = df[df['outlier_type'] == outlier_type]
            type_name = _get_outlier_type_name(outlier_type)
            filepath = os.path.join(output_dir, f'list_posts_{outlier_type}.{fmt}')
            _save_dataframe(type_df, filepath, fmt)
            files_created.append(filepath)
            print_info(f"  â””â”€ {type_name}: {os.path.basename(filepath)} ({len(type_df)} æ¡)")
    
    return df, files_created


def export_activity_stats(connection, session_id: str, output_dir: str, fmt: str = 'csv') -> Tuple[pd.DataFrame, List[str]]:
    """å¯¼å‡ºæ´»è·ƒåº¦ç»Ÿè®¡æ•°æ®"""
    query = """
    SELECT 
        stat_type,
        time_key,
        activity_count,
        activity_percentage,
        analysis_timestamp
    FROM activity_stats
    WHERE session_id = %s
    ORDER BY stat_type, CAST(time_key AS UNSIGNED)
    """
    
    df = pd.read_sql(query, connection, params=[session_id])
    files_created = []
    
    if not df.empty:
        # æŒ‰ç»Ÿè®¡ç±»å‹åˆ†åˆ«å¯¼å‡º
        for stat_type in df['stat_type'].unique():
            type_df = df[df['stat_type'] == stat_type]
            type_name = _get_stat_type_name(stat_type)
            filepath = os.path.join(output_dir, f'stats_{stat_type}.{fmt}')
            _save_dataframe(type_df, filepath, fmt)
            files_created.append(filepath)
            print_success(f"å¯¼å‡º{type_name}: {os.path.basename(filepath)}")
    
    return df, files_created


def export_potential_new_users(connection, session_id: str, output_dir: str, fmt: str = 'csv') -> Tuple[pd.DataFrame, List[str]]:
    """å¯¼å‡ºæ½œåœ¨æ–°ç”¨æˆ·æ•°æ®"""
    query = """
    SELECT 
        username,
        weighted_reply_score,
        reply_count,
        avg_replier_pagerank,
        analysis_timestamp
    FROM potential_new_users
    WHERE session_id = %s
    ORDER BY weighted_reply_score DESC
    """
    
    df = pd.read_sql(query, connection, params=[session_id])
    files_created = []
    
    if not df.empty:
        filepath = os.path.join(output_dir, f'watchlist_potential_new_users.{fmt}')
        _save_dataframe(df, filepath, fmt)
        files_created.append(filepath)
        print_success(f"å¯¼å‡ºæ½œåœ¨æ–°ç”¨æˆ·: {os.path.basename(filepath)} ({len(df)} ä¸ª)")
    
    return df, files_created


def export_llm_outputs(connection, session_id: str, output_dir: str, fmt: str = 'json') -> Tuple[pd.DataFrame, List[str]]:
    """å¯¼å‡º LLM è¾“å‡ºè®°å½•"""
    query = """
    SELECT 
        task_type,
        target_id,
        model_used,
        prompt_tokens,
        completion_tokens,
        total_cost,
        parsed_output,
        created_at
    FROM llm_outputs
    WHERE session_id = %s
    ORDER BY created_at DESC
    """
    
    df = pd.read_sql(query, connection, params=[session_id])
    files_created = []
    
    if not df.empty:
        # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„å¯¼å‡º
        llm_insights_dir = os.path.join(output_dir, 'llm_insights')
        os.makedirs(llm_insights_dir, exist_ok=True)
        
        for task_type in df['task_type'].unique():
            type_df = df[df['task_type'] == task_type]
            
            # æå–å¹¶åˆå¹¶ parsed_output
            outputs = []
            for _, row in type_df.iterrows():
                parsed = row['parsed_output']
                if parsed:
                    if isinstance(parsed, str):
                        try:
                            parsed = json.loads(parsed)
                        except:
                            pass
                    if isinstance(parsed, dict):
                        parsed['_model_used'] = row['model_used']
                        parsed['_created_at'] = str(row['created_at'])
                        outputs.append(parsed)
            
            if outputs:
                task_name = _get_task_type_name(task_type)
                filepath = os.path.join(llm_insights_dir, f'{task_type}.json')
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(outputs, f, ensure_ascii=False, indent=2, default=str)
                files_created.append(filepath)
                print_info(f"  â””â”€ {task_name}: {len(outputs)} æ¡")
        
        print_success(f"å¯¼å‡º LLM æ´å¯Ÿåˆ° llm_insights/ ç›®å½•")
    
    return df, files_created


def export_user_strategy_dossiers(connection, session_id: str, output_dir: str, fmt: str = 'json') -> Tuple[pd.DataFrame, List[str]]:
    """å¯¼å‡ºç”¨æˆ·ç­–ç•¥ç”»åƒ"""
    query = """
    SELECT 
        username,
        core_identity,
        growth_tactics,
        monetization_model,
        content_style_summary,
        actionable_takeaways,
        model_used,
        created_at
    FROM user_strategy_dossiers
    WHERE session_id = %s
    ORDER BY created_at DESC
    """
    
    df = pd.read_sql(query, connection, params=[session_id])
    files_created = []
    
    if not df.empty:
        llm_insights_dir = os.path.join(output_dir, 'llm_insights')
        os.makedirs(llm_insights_dir, exist_ok=True)
        
        filepath = os.path.join(llm_insights_dir, 'User_Strategy_Dossiers.json')
        _save_json_from_df(df, filepath, ['growth_tactics', 'actionable_takeaways'])
        files_created.append(filepath)
        print_success(f"å¯¼å‡ºç”¨æˆ·ç­–ç•¥ç”»åƒ: {os.path.basename(filepath)} ({len(df)} æ¡)")
    
    return df, files_created


def export_content_blueprints(connection, session_id: str, output_dir: str, fmt: str = 'json') -> Tuple[pd.DataFrame, List[str]]:
    """å¯¼å‡ºçˆ†æ¬¾å†…å®¹è“å›¾"""
    query = """
    SELECT 
        source_tweet_id,
        quadrant,
        hook_style,
        body_structure,
        readability_features,
        emotional_tone,
        call_to_action,
        why_viral,
        replication_template,
        model_used,
        created_at
    FROM content_blueprints
    WHERE session_id = %s
    ORDER BY created_at DESC
    """
    
    df = pd.read_sql(query, connection, params=[session_id])
    files_created = []
    
    if not df.empty:
        llm_insights_dir = os.path.join(output_dir, 'llm_insights')
        os.makedirs(llm_insights_dir, exist_ok=True)
        
        filepath = os.path.join(llm_insights_dir, 'viral_deconstruction.json')
        _save_json_from_df(df, filepath, ['hook_style', 'readability_features', 'call_to_action'])
        files_created.append(filepath)
        print_success(f"å¯¼å‡ºçˆ†æ¬¾å†…å®¹è“å›¾: {os.path.basename(filepath)} ({len(df)} æ¡)")
    
    return df, files_created


def export_content_idea_bank(connection, session_id: str, output_dir: str, fmt: str = 'json') -> Tuple[pd.DataFrame, List[str]]:
    """å¯¼å‡ºå†…å®¹åˆ›æ„åº“"""
    query = """
    SELECT 
        source_tweet_id,
        idea_type,
        topic,
        user_intent,
        suggested_angle,
        suggested_title,
        status,
        model_used,
        created_at
    FROM content_idea_bank
    WHERE session_id = %s
    ORDER BY created_at DESC
    """
    
    df = pd.read_sql(query, connection, params=[session_id])
    files_created = []
    
    if not df.empty:
        llm_insights_dir = os.path.join(output_dir, 'llm_insights')
        os.makedirs(llm_insights_dir, exist_ok=True)
        
        filepath = os.path.join(llm_insights_dir, 'Content_Idea_Bank.json')
        _save_json_from_df(df, filepath, [])
        files_created.append(filepath)
        print_success(f"å¯¼å‡ºå†…å®¹åˆ›æ„åº“: {os.path.basename(filepath)} ({len(df)} æ¡)")
    
    return df, files_created


def export_post_features(connection, session_id: str, output_dir: str, fmt: str = 'csv') -> Tuple[pd.DataFrame, List[str]]:
    """å¯¼å‡ºæ¨æ–‡ç‰¹å¾æ•°æ®"""
    query = """
    SELECT 
        tweet_id,
        conversation_id,
        utility_score,
        discussion_rate,
        virality_rate,
        is_question,
        sentiment_score,
        asset_quadrant,
        thread_retention_rate,
        thread_length,
        funnel_signal,
        analysis_timestamp
    FROM post_features
    WHERE session_id = %s
    ORDER BY utility_score DESC
    """
    
    df = pd.read_sql(query, connection, params=[session_id])
    files_created = []
    
    if not df.empty:
        filepath = os.path.join(output_dir, f'post_features.{fmt}')
        _save_dataframe(df, filepath, fmt)
        files_created.append(filepath)
        print_success(f"å¯¼å‡ºæ¨æ–‡ç‰¹å¾: {os.path.basename(filepath)} ({len(df)} æ¡)")
        
        # å†…å®¹èµ„äº§å››è±¡é™åˆ†æ
        if 'asset_quadrant' in df.columns:
            quadrant_stats = df['asset_quadrant'].value_counts().to_dict()
            if quadrant_stats:
                print_info(f"  â””â”€ å†…å®¹å››è±¡é™åˆ†å¸ƒ: {quadrant_stats}")
    
    return df, files_created


def export_content_efficiency(connection, session_id: str, output_dir: str, fmt: str = 'csv') -> Tuple[pd.DataFrame, List[str]]:
    """å¯¼å‡ºå†…å®¹æ•ˆèƒ½ç»Ÿè®¡"""
    query = """
    SELECT 
        media_type,
        post_count,
        avg_views,
        avg_likes,
        avg_replies,
        avg_bookmarks,
        avg_utility_score,
        analysis_timestamp
    FROM content_efficiency
    WHERE session_id = %s
    ORDER BY post_count DESC
    """
    
    df = pd.read_sql(query, connection, params=[session_id])
    files_created = []
    
    if not df.empty:
        filepath = os.path.join(output_dir, f'stats_content_efficiency.{fmt}')
        _save_dataframe(df, filepath, fmt)
        files_created.append(filepath)
        print_success(f"å¯¼å‡ºå†…å®¹æ•ˆèƒ½ç»Ÿè®¡: {os.path.basename(filepath)} ({len(df)} ç§åª’ä½“ç±»å‹)")
    
    return df, files_created


# =====================================================
# è¾…åŠ©å‡½æ•°
# =====================================================
def _save_dataframe(df: pd.DataFrame, filepath: str, fmt: str):
    """ä¿å­˜ DataFrame åˆ°æ–‡ä»¶"""
    if fmt == 'csv':
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
    elif fmt == 'json':
        df.to_json(filepath, orient='records', force_ascii=False, indent=2)
    elif fmt == 'markdown':
        with open(filepath.replace('.markdown', '.md'), 'w', encoding='utf-8') as f:
            f.write(df.to_markdown(index=False))
    else:
        df.to_csv(filepath, index=False, encoding='utf-8-sig')


def _save_json_from_df(df: pd.DataFrame, filepath: str, json_columns: List[str]):
    """å°† DataFrame ä¿å­˜ä¸º JSONï¼Œè‡ªåŠ¨è§£æ JSON å­—æ®µ"""
    records = []
    for _, row in df.iterrows():
        record = row.to_dict()
        for col in json_columns:
            if col in record and record[col]:
                if isinstance(record[col], str):
                    try:
                        record[col] = json.loads(record[col])
                    except:
                        pass
        records.append(record)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(records, f, ensure_ascii=False, indent=2, default=str)


def _get_outlier_type_name(outlier_type: str) -> str:
    """è·å–å¼‚å¸¸ç±»å‹çš„ä¸­æ–‡åç§°"""
    names = {
        'high_utility': 'é«˜å¹²è´§å†…å®¹',
        'high_traffic': 'é«˜æµé‡å†…å®¹',
        'high_discussion': 'é«˜è®¨è®ºå†…å®¹',
        'unanswered_question': 'æœªå›ç­”é—®é¢˜',
        'hot_debate': 'çƒ­è®®è¯é¢˜'
    }
    return names.get(outlier_type, outlier_type)


def _get_stat_type_name(stat_type: str) -> str:
    """è·å–ç»Ÿè®¡ç±»å‹çš„ä¸­æ–‡åç§°"""
    names = {
        'hourly_heatmap': 'å°æ—¶æ´»è·ƒçƒ­åŠ›å›¾',
        'daily_trend': 'æ—¥æ´»è·ƒè¶‹åŠ¿',
        'weekly_pattern': 'å‘¨æ´»è·ƒæ¨¡å¼'
    }
    return names.get(stat_type, stat_type)


def _get_task_type_name(task_type: str) -> str:
    """è·å–ä»»åŠ¡ç±»å‹çš„ä¸­æ–‡åç§°"""
    names = {
        'viral_deconstruction': 'çˆ†æ¬¾å†…å®¹æ‹†è§£',
        'user_profiling': 'ç”¨æˆ·ç­–ç•¥ç”»åƒ',
        'relationship_insight': 'å…³ç³»æ´å¯Ÿ',
        'content_opportunity': 'å†…å®¹æœºä¼šæŒ–æ˜',
        'thread_analysis': 'Threadåˆ†æ',
        'monetization_analysis': 'å˜ç°æ¨¡å¼åˆ†æ'
    }
    return names.get(task_type, task_type)


def generate_summary_report(session_info: Dict, stats: Dict, output_dir: str, files_created: List[str]) -> str:
    """ç”Ÿæˆä¼šè¯æ‘˜è¦æŠ¥å‘Š (Markdown æ ¼å¼)"""
    report_lines = []
    
    # æ ‡é¢˜
    session_id = session_info.get('session_id', 'Unknown')
    report_lines.append(f"# ğŸ” X_deepdive åˆ†æç»“æœæŠ¥å‘Š\n")
    report_lines.append(f"**ä¼šè¯ ID**: `{session_id}`\n")
    report_lines.append(f"**å¯¼å‡ºæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # ä¼šè¯ä¿¡æ¯
    report_lines.append("\n## ğŸ“Š ä¼šè¯ä¿¡æ¯\n")
    report_lines.append(f"| å±æ€§ | å€¼ |")
    report_lines.append(f"|------|-----|")
    report_lines.append(f"| å¼€å§‹æ—¶é—´ | {session_info.get('started_at', 'N/A')} |")
    report_lines.append(f"| å®Œæˆæ—¶é—´ | {session_info.get('completed_at', 'N/A')} |")
    report_lines.append(f"| çŠ¶æ€ | {session_info.get('status', 'N/A')} |")
    
    # æ•°æ®ç»Ÿè®¡
    report_lines.append("\n## ğŸ“ˆ æ•°æ®ç»Ÿè®¡\n")
    report_lines.append(f"| æ•°æ®ç±»å‹ | æ•°é‡ |")
    report_lines.append(f"|----------|------|")
    for name, count in stats.items():
        if count > 0:
            report_lines.append(f"| {name} | {count:,} |")
    
    # å¯¼å‡ºæ–‡ä»¶æ¸…å•
    report_lines.append("\n## ğŸ“ å¯¼å‡ºæ–‡ä»¶æ¸…å•\n")
    
    # æŒ‰ç›®å½•åˆ†ç»„
    csv_files = [f for f in files_created if f.endswith('.csv')]
    json_files = [f for f in files_created if f.endswith('.json')]
    other_files = [f for f in files_created if not f.endswith('.csv') and not f.endswith('.json')]
    
    if csv_files:
        report_lines.append("\n### CSV æ–‡ä»¶ (ç»“æ„åŒ–æ•°æ®)\n")
        for f in csv_files:
            report_lines.append(f"- `{os.path.basename(f)}`")
    
    if json_files:
        report_lines.append("\n### JSON æ–‡ä»¶ (LLM æ´å¯Ÿ)\n")
        for f in json_files:
            report_lines.append(f"- `{os.path.basename(f)}`")
    
    if other_files:
        report_lines.append("\n### å…¶ä»–æ–‡ä»¶\n")
        for f in other_files:
            report_lines.append(f"- `{os.path.basename(f)}`")
    
    # æ•°æ®è¯´æ˜
    report_lines.append("\n## ğŸ“– æ•°æ®è¯´æ˜\n")
    report_lines.append("""
### ç”¨æˆ·åˆ†ææ–‡ä»¶

- **`all_users_with_metrics.csv`**: æ‰€æœ‰ç”¨æˆ·çš„å®Œæ•´æŒ‡æ ‡æ•°æ®
- **`watchlist_authorities.csv`**: æƒå¨æ¢çº½ Top 50 (æŒ‰ PageRank æ’åº)
- **`watchlist_connectors.csv`**: ç ´åœˆè€… Top 50 (æŒ‰ Betweenness æ’åº)
- **`watchlist_rising_stars.csv`**: å´›èµ·æ–°æ˜Ÿ Top 50 (æŒ‰å¢é•¿é€Ÿåº¦æ’åº)
- **`watchlist_potential_new_users.csv`**: å€¼å¾—å…³æ³¨çš„æ½œåœ¨æ–°ç”¨æˆ·

### å†…å®¹åˆ†ææ–‡ä»¶

- **`list_posts_outliers.csv`**: é«˜ä»·å€¼å¼‚å¸¸å†…å®¹æ±‡æ€»
- **`list_posts_high_utility.csv`**: é«˜å¹²è´§å†…å®¹ (æ”¶è—/ç‚¹èµæ¯”é«˜)
- **`list_posts_unanswered_question.csv`**: æœªè¢«å……åˆ†å›ç­”çš„é—®é¢˜ (å†…å®¹æœºä¼š)

### ç¤¾ç¾¤åˆ†ææ–‡ä»¶

- **`community_stats.csv`**: ç¤¾ç¾¤ç»Ÿè®¡æ•°æ®
- **`list_interactions_strong_ties.csv`**: å¼ºäº’æƒ å…³ç³»å¯¹

### LLM æ´å¯Ÿæ–‡ä»¶ (llm_insights/)

- **`User_Strategy_Dossiers.json`**: æˆåŠŸç”¨æˆ·çš„ç­–ç•¥ç”»åƒ
- **`Content_Blueprints.json`**: çˆ†æ¬¾å†…å®¹çš„é€†å‘å·¥ç¨‹åˆ†æ
- **`Content_Idea_Bank.json`**: å†…å®¹åˆ›æ„å’Œé€‰é¢˜åº“

è¯¦ç»†å­—æ®µè¯´æ˜è¯·å‚è€ƒ `docs/EXPORT_RESULTS_GUIDE.md`
""")
    
    report_content = '\n'.join(report_lines)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = os.path.join(output_dir, 'EXPORT_SUMMARY.md')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    return report_path


# =====================================================
# äº¤äº’å¼èœå•
# =====================================================
def show_interactive_menu(connection, session_id: str) -> List[str]:
    """æ˜¾ç¤ºäº¤äº’å¼å¯¼å‡ºèœå•"""
    print_section("é€‰æ‹©è¦å¯¼å‡ºçš„æ•°æ®ç±»å‹", "ğŸ“‹")
    
    options = [
        ('1', 'ç”¨æˆ·æŒ‡æ ‡æ•°æ®', 'user_metrics', 'åŒ…å« PageRank, Betweenness, ç¤¾ç¾¤ID ç­‰'),
        ('2', 'ç¤¾ç¾¤ç»Ÿè®¡æ•°æ®', 'community', 'å„ç¤¾ç¾¤çš„æˆå‘˜æ•°ã€æ ¸å¿ƒæˆå‘˜ç­‰'),
        ('3', 'å¼ºäº’æƒ å…³ç³»', 'ties', 'é«˜é¢‘äº’åŠ¨çš„ç”¨æˆ·å¯¹'),
        ('4', 'é«˜ä»·å€¼å†…å®¹', 'outliers', 'å¹²è´§å†…å®¹ã€çƒ­è®®è¯é¢˜ã€æœªå›ç­”é—®é¢˜ç­‰'),
        ('5', 'æ´»è·ƒåº¦ç»Ÿè®¡', 'activity', 'æŒ‰å°æ—¶/å¤©çš„æ´»è·ƒåˆ†å¸ƒ'),
        ('6', 'æ½œåœ¨æ–°ç”¨æˆ·', 'potential', 'å€¼å¾—å…³æ³¨çš„æ–°ç”¨æˆ·'),
        ('7', 'LLM æ´å¯ŸæŠ¥å‘Š', 'llm', 'ç­–ç•¥ç”»åƒã€çˆ†æ¬¾è“å›¾ã€åˆ›æ„åº“'),
        ('8', 'æ¨æ–‡ç‰¹å¾æ•°æ®', 'features', 'å†…å®¹å››è±¡é™ã€Threadåˆ†æç­‰'),
        ('9', 'å†…å®¹æ•ˆèƒ½ç»Ÿè®¡', 'efficiency', 'ä¸åŒåª’ä½“ç±»å‹çš„æ•ˆèƒ½å¯¹æ¯”'),
        ('A', 'å…¨éƒ¨å¯¼å‡º', 'all', 'å¯¼å‡ºä»¥ä¸Šæ‰€æœ‰æ•°æ®'),
    ]
    
    for opt, name, _, desc in options:
        print(f"  {Colors.CYAN}[{opt}]{Colors.ENDC} {name}")
        print(f"      {Colors.BLUE}{desc}{Colors.ENDC}")
    
    print(f"\n  {Colors.CYAN}[Q]{Colors.ENDC} é€€å‡º")
    
    print(f"\n{Colors.YELLOW}è¯·è¾“å…¥é€‰é¡¹ (å¤šé€‰ç”¨é€—å·åˆ†éš”ï¼Œå¦‚ 1,2,3): {Colors.ENDC}", end='')
    
    try:
        choice = input().strip().upper()
    except (EOFError, KeyboardInterrupt):
        return []
    
    if choice == 'Q':
        return []
    
    if choice == 'A':
        return ['all']
    
    selected = []
    for c in choice.split(','):
        c = c.strip()
        for opt, _, key, _ in options:
            if c == opt:
                selected.append(key)
                break
    
    return selected if selected else ['all']


def display_session_selector(connection) -> Optional[str]:
    """æ˜¾ç¤ºä¼šè¯é€‰æ‹©å™¨"""
    sessions_df = list_sessions(connection)
    
    if sessions_df.empty:
        print_error("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†æä¼šè¯")
        return None
    
    print_section("é€‰æ‹©è¦å¯¼å‡ºçš„åˆ†æä¼šè¯", "ğŸ“…")
    
    print(f"\n{'åºå·':<4} {'ä¼šè¯ID':<20} {'å¼€å§‹æ—¶é—´':<20} {'çŠ¶æ€':<12}")
    print("-" * 60)
    
    for idx, row in sessions_df.iterrows():
        status_color = Colors.GREEN if row['status'] == 'completed' else Colors.YELLOW
        print(f"{idx+1:<4} {row['session_id']:<20} {str(row['started_at']):<20} {status_color}{row['status']:<12}{Colors.ENDC}")
    
    print(f"\n{Colors.YELLOW}è¾“å…¥åºå·é€‰æ‹©ä¼šè¯ (ç›´æ¥å›è½¦é€‰æ‹©æœ€æ–°çš„): {Colors.ENDC}", end='')
    
    try:
        choice = input().strip()
    except (EOFError, KeyboardInterrupt):
        return None
    
    if not choice:
        # è¿”å›æœ€æ–°çš„å·²å®Œæˆä¼šè¯
        completed = sessions_df[sessions_df['status'] == 'completed']
        if completed.empty:
            return sessions_df.iloc[0]['session_id']
        return completed.iloc[0]['session_id']
    
    try:
        idx = int(choice) - 1
        if 0 <= idx < len(sessions_df):
            return sessions_df.iloc[idx]['session_id']
    except ValueError:
        # å¯èƒ½ç›´æ¥è¾“å…¥äº†ä¼šè¯ID
        if choice in sessions_df['session_id'].values:
            return choice
    
    print_warning("æ— æ•ˆçš„é€‰æ‹©ï¼Œä½¿ç”¨æœ€æ–°ä¼šè¯")
    return sessions_df.iloc[0]['session_id']


def select_output_format() -> str:
    """é€‰æ‹©è¾“å‡ºæ ¼å¼"""
    print(f"\n{Colors.YELLOW}é€‰æ‹©å¯¼å‡ºæ ¼å¼ [csv/json] (é»˜è®¤ csv): {Colors.ENDC}", end='')
    
    try:
        choice = input().strip().lower()
    except (EOFError, KeyboardInterrupt):
        return 'csv'
    
    return choice if choice in ['csv', 'json'] else 'csv'


# =====================================================
# ä¸»å¯¼å‡ºå‡½æ•°
# =====================================================
def export_all(session_id: str, output_dir: str, fmt: str = 'csv', 
               export_types: Optional[List[str]] = None) -> bool:
    """
    å¯¼å‡ºæ‰€æœ‰æ•°æ®
    
    Args:
        session_id: ä¼šè¯ID
        output_dir: è¾“å‡ºç›®å½•
        fmt: å¯¼å‡ºæ ¼å¼ (csv/json)
        export_types: è¦å¯¼å‡ºçš„æ•°æ®ç±»å‹åˆ—è¡¨ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
    """
    connection = get_analysis_connection()
    if not connection:
        print_error("æ— æ³•è¿æ¥åˆ°åˆ†ææ•°æ®åº“")
        return False
    
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_subdir = os.path.join(output_dir, f'export_{session_id}_{timestamp}')
        os.makedirs(output_subdir, exist_ok=True)
        
        print_section(f"å¼€å§‹å¯¼å‡ºåˆ†æä¼šè¯: {session_id}", "ğŸš€")
        print_info(f"è¾“å‡ºç›®å½•: {output_subdir}")
        print_info(f"å¯¼å‡ºæ ¼å¼: {fmt.upper()}")
        
        all_files_created = []
        export_all_types = export_types is None or 'all' in export_types
        
        # è·å–ä¼šè¯ä¿¡æ¯å’Œç»Ÿè®¡
        session_info = get_session_info(connection, session_id) or {'session_id': session_id}
        stats = get_session_stats(connection, session_id)
        
        # æŒ‰ç±»å‹å¯¼å‡º
        if export_all_types or 'user_metrics' in export_types:
            print_section("ç”¨æˆ·æŒ‡æ ‡æ•°æ®", "ğŸ‘¥")
            _, files = export_user_metrics(connection, session_id, output_subdir, fmt)
            all_files_created.extend(files)
        
        if export_all_types or 'community' in export_types:
            print_section("ç¤¾ç¾¤ç»Ÿè®¡æ•°æ®", "ğŸ˜ï¸")
            _, files = export_community_stats(connection, session_id, output_subdir, fmt)
            all_files_created.extend(files)
        
        if export_all_types or 'ties' in export_types:
            print_section("å¼ºäº’æƒ å…³ç³»", "ğŸ¤")
            _, files = export_strong_ties(connection, session_id, output_subdir, fmt)
            all_files_created.extend(files)
        
        if export_all_types or 'outliers' in export_types:
            print_section("é«˜ä»·å€¼å†…å®¹", "ğŸ’")
            _, files = export_content_outliers(connection, session_id, output_subdir, fmt)
            all_files_created.extend(files)
        
        if export_all_types or 'activity' in export_types:
            print_section("æ´»è·ƒåº¦ç»Ÿè®¡", "ğŸ“ˆ")
            _, files = export_activity_stats(connection, session_id, output_subdir, fmt)
            all_files_created.extend(files)
        
        if export_all_types or 'potential' in export_types:
            print_section("æ½œåœ¨æ–°ç”¨æˆ·", "ğŸŒŸ")
            _, files = export_potential_new_users(connection, session_id, output_subdir, fmt)
            all_files_created.extend(files)
        
        if export_all_types or 'llm' in export_types:
            print_section("LLM æ´å¯ŸæŠ¥å‘Š", "ğŸ¤–")
            _, files = export_llm_outputs(connection, session_id, output_subdir)
            all_files_created.extend(files)
            _, files = export_user_strategy_dossiers(connection, session_id, output_subdir)
            all_files_created.extend(files)
            _, files = export_content_blueprints(connection, session_id, output_subdir)
            all_files_created.extend(files)
            _, files = export_content_idea_bank(connection, session_id, output_subdir)
            all_files_created.extend(files)
        
        if export_all_types or 'features' in export_types:
            print_section("æ¨æ–‡ç‰¹å¾æ•°æ®", "ğŸ“")
            _, files = export_post_features(connection, session_id, output_subdir, fmt)
            all_files_created.extend(files)
        
        if export_all_types or 'efficiency' in export_types:
            print_section("å†…å®¹æ•ˆèƒ½ç»Ÿè®¡", "ğŸ“Š")
            _, files = export_content_efficiency(connection, session_id, output_subdir, fmt)
            all_files_created.extend(files)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        print_section("ç”Ÿæˆå¯¼å‡ºæŠ¥å‘Š", "ğŸ“‹")
        report_path = generate_summary_report(session_info, stats, output_subdir, all_files_created)
        print_success(f"ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š: {os.path.basename(report_path)}")
        
        # æ‰“å°å®Œæˆä¿¡æ¯
        print(f"\n{Colors.GREEN}{Colors.BOLD}")
        print("â•" * 60)
        print(f"  âœ… å¯¼å‡ºå®Œæˆ!")
        print(f"  ğŸ“ è¾“å‡ºç›®å½•: {output_subdir}")
        print(f"  ğŸ“„ æ–‡ä»¶æ•°é‡: {len(all_files_created) + 1}")
        print("â•" * 60)
        print(f"{Colors.ENDC}")
        
        return True
    
    except Exception as e:
        print_error(f"å¯¼å‡ºå¤±è´¥: {e}")
        logger.exception("å¯¼å‡ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯")
        return False
    
    finally:
        connection.close()


# =====================================================
# ä¸»å‡½æ•°
# =====================================================
def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='X_deepdive åˆ†æç»“æœå¯¼å‡ºå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python -m src.export_results                    # äº¤äº’å¼èœå•
  python -m src.export_results --list             # åˆ—å‡ºæ‰€æœ‰ä¼šè¯
  python -m src.export_results --latest           # å¯¼å‡ºæœ€æ–°ä¼šè¯
  python -m src.export_results --session 20231201_120000 --output ./my_exports
  python -m src.export_results --all --format json
        """
    )
    parser.add_argument('--session', type=str, help='æŒ‡å®šè¦å¯¼å‡ºçš„ä¼šè¯ID')
    parser.add_argument('--latest', action='store_true', help='å¯¼å‡ºæœ€æ–°å®Œæˆçš„ä¼šè¯')
    parser.add_argument('--list', action='store_true', help='åˆ—å‡ºæ‰€æœ‰åˆ†æä¼šè¯')
    parser.add_argument('--all', action='store_true', help='éäº¤äº’å¼å¯¼å‡ºæ‰€æœ‰æ•°æ®')
    parser.add_argument('--output', type=str, default='./exports', help='è¾“å‡ºç›®å½• (é»˜è®¤: ./exports)')
    parser.add_argument('--format', type=str, choices=['csv', 'json'], default='csv', 
                       help='å¯¼å‡ºæ ¼å¼ (é»˜è®¤: csv)')
    parser.add_argument('--no-color', action='store_true', help='ç¦ç”¨å½©è‰²è¾“å‡º')
    
    args = parser.parse_args()
    
    # å¤„ç†é¢œè‰²
    if args.no_color or not sys.stdout.isatty():
        Colors.disable()
    
    # æ‰“å°æ¬¢è¿æ¨ªå¹…
    print_banner()
    
    # è¿æ¥æ•°æ®åº“
    connection = get_analysis_connection()
    if not connection:
        print_error("æ— æ³•è¿æ¥åˆ°åˆ†ææ•°æ®åº“")
        print_info("è¯·æ£€æŸ¥ config.ini ä¸­çš„ ANALYSIS_DATABASE_URL é…ç½®")
        print_info("æˆ–è®¾ç½®ç¯å¢ƒå˜é‡: export ANALYSIS_DATABASE_URL='mysql://user:pass@host:port/dbname'")
        sys.exit(1)
    
    try:
        # åˆ—å‡ºä¼šè¯æ¨¡å¼
        if args.list:
            print_section("åˆ†æä¼šè¯åˆ—è¡¨", "ğŸ“…")
            sessions = list_sessions(connection)
            if sessions.empty:
                print_warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†æä¼šè¯")
            else:
                print(sessions.to_string(index=False))
                print(f"\n{Colors.BLUE}å…± {len(sessions)} ä¸ªä¼šè¯{Colors.ENDC}")
            return
        
        # ç¡®å®šä¼šè¯ID
        session_id = args.session
        
        if args.latest:
            session_id = get_latest_session(connection)
            if not session_id:
                print_error("æ²¡æœ‰æ‰¾åˆ°å·²å®Œæˆçš„åˆ†æä¼šè¯")
                sys.exit(1)
            print_info(f"ä½¿ç”¨æœ€æ–°ä¼šè¯: {session_id}")
        
        # äº¤äº’å¼æ¨¡å¼
        if not session_id and not args.all:
            session_id = display_session_selector(connection)
            if not session_id:
                print_info("å·²å–æ¶ˆæ“ä½œ")
                return
        
        # å¦‚æœä»ç„¶æ²¡æœ‰ä¼šè¯IDï¼Œå°è¯•è·å–æœ€æ–°çš„
        if not session_id:
            session_id = get_latest_session(connection)
            if not session_id:
                print_error("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ç”¨çš„åˆ†æä¼šè¯")
                sys.exit(1)
        
        # æ˜¾ç¤ºä¼šè¯ç»Ÿè®¡
        stats = get_session_stats(connection, session_id)
        print_section(f"ä¼šè¯ {session_id} æ•°æ®æ¦‚è§ˆ", "ğŸ“Š")
        for name, count in stats.items():
            if count > 0:
                print(f"  {name}: {Colors.CYAN}{count:,}{Colors.ENDC} æ¡")
        
        # éäº¤äº’å¼å…¨é‡å¯¼å‡º
        if args.all:
            connection.close()
            success = export_all(session_id, args.output, args.format, ['all'])
            sys.exit(0 if success else 1)
        
        # äº¤äº’å¼é€‰æ‹©å¯¼å‡ºç±»å‹
        export_types = show_interactive_menu(connection, session_id)
        if not export_types:
            print_info("å·²å–æ¶ˆæ“ä½œ")
            return
        
        # é€‰æ‹©å¯¼å‡ºæ ¼å¼
        fmt = select_output_format() if not args.format else args.format
        
        connection.close()
        
        # æ‰§è¡Œå¯¼å‡º
        success = export_all(session_id, args.output, fmt, export_types)
        sys.exit(0 if success else 1)
    
    except KeyboardInterrupt:
        print(f"\n{Colors.YELLOW}æ“ä½œå·²å–æ¶ˆ{Colors.ENDC}")
        sys.exit(0)
    except Exception as e:
        print_error(f"æ“ä½œå¤±è´¥: {e}")
        logger.exception("å‘ç”Ÿé”™è¯¯")
        sys.exit(1)
    finally:
        if connection and connection.open:
            connection.close()


if __name__ == '__main__':
    main()
